# NOTE: This Dockerfile is based on CUDA 12.1.
# To benchmark on other CUDA versions, search and replace "12.1" and "121".
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

SHELL ["/bin/bash", "-ec"]

# Step 1. Set up bashrc
RUN grep -v '[ -z "\$PS1" ] && return' ~/.bashrc >/tmp/bashrc && \
    echo "alias conda=micromamba" >>/tmp/bashrc               && \
    mv /tmp/bashrc ~/.bashrc

RUN apt update                                                          && \
    apt install --yes wget curl git vim build-essential openssh-server

RUN git clone https://github.com/turboderp/exllamav2.git /exllamav2     && \
    echo "export PYTHONPATH=\"/exllamav2/:$PYTHONPATH\"" >> ~/.bashrc

RUN bash <(curl -L micro.mamba.pm/install.sh) && source ~/.bashrc           && \
    micromamba create --yes -n python311                                       \
      -c pytorch-nightly -c nvidia -c conda-forge                              \
      python=3.11                                                              \
      pytorch "pytorch-cuda==12.1" "cuda-toolkit==12.1" "cuda-version==12.1"   \
      packaging ninja fastparquet pandas pygments websockets                   \
      "sentencepiece>=0.1.97" "safetensors>=0.3.2"
